{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Neural machine translation with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xh8WNEwYA3BW"
   },
   "source": [
    "This implemetation demonstrates how to train a sequence-to-sequence (seq2seq) model for Gloss-to-Text English translation roughly based on [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025v5) (Luong et al., 2015). \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN%2Battention-words-spa.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <th colspan=1>An encoder/decoder connected by attention.</th>\n",
    "<tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAmSR1FaqKrl"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:20.480963Z",
     "iopub.status.busy": "2022-12-14T13:53:20.480297Z",
     "iopub.status.idle": "2022-12-14T13:53:24.238557Z",
     "shell.execute_reply": "2022-12-14T13:53:24.237691Z"
    },
    "id": "DGFTkuRvzWqc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-text>=2.10 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow<2.12,>=2.11.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-text>=2.10) (2.11.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-text>=2.10) (0.12.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.32.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (3.14.0)\n",
      "Requirement already satisfied: setuptools in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (58.0.4)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (14.0.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (4.4.0)\n",
      "Requirement already satisfied: packaging in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (21.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.12.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (22.10.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (0.3.3)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.21.2)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (2.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (2.11.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.16.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (0.27.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (0.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (0.37.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (2.0.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (0.4.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (3.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.8.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.26.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (0.6.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from packaging->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (3.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (4.7)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (2022.9.24)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (3.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages (0.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"tensorflow-text>=2.10\"\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:24.243288Z",
     "iopub.status.busy": "2022-12-14T13:53:24.242555Z",
     "iopub.status.idle": "2022-12-14T13:53:26.697361Z",
     "shell.execute_reply": "2022-12-14T13:53:26.696646Z"
    },
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 03:22:19.953892: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:26.701805Z",
     "iopub.status.busy": "2022-12-14T13:53:26.700963Z",
     "iopub.status.idle": "2022-12-14T13:53:26.706955Z",
     "shell.execute_reply": "2022-12-14T13:53:26.706105Z"
    },
    "id": "KqFqKi4fqN9X"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "class ShapeChecker():\n",
    "  def __init__(self):\n",
    "    # Keep a cache of every axis-name seen\n",
    "    self.shapes = {}\n",
    "\n",
    "  def __call__(self, tensor, names, broadcast=False):\n",
    "    if not tf.executing_eagerly():\n",
    "      return\n",
    "\n",
    "    parsed = einops.parse_shape(tensor, names)\n",
    "\n",
    "    for name, new_dim in parsed.items():\n",
    "      old_dim = self.shapes.get(name, None)\n",
    "      \n",
    "      if (broadcast and new_dim == 1):\n",
    "        continue\n",
    "\n",
    "      if old_dim is None:\n",
    "        # If the axis name is new, add its length to the cache.\n",
    "        self.shapes[name] = new_dim\n",
    "        continue\n",
    "\n",
    "      if new_dim != old_dim:\n",
    "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                         f\"    found: {new_dim}\\n\"\n",
    "                         f\"    expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjUROhJfH3ML"
   },
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puE_K74DIE9W"
   },
   "source": [
    "The tutorial uses a Statistical Machine Translation for Sign Language dataset provided by Dr. Achraf Othman (ASLG_PC12). This dataset contains language translation pairs in the format: \n",
    "\n",
    "```\n",
    "APPROVAL MINUTE DESC-PREVIOUS SIT SEE MINUTE\tapproval of minutes of previous sitting see minutes\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfodePkj3jEa"
   },
   "source": [
    "### Download and prepare the dataset\n",
    "\n",
    "Here are the steps you need to take to prepare the data:\n",
    "\n",
    "1. Add a *start* and *end* token to each sentence.\n",
    "2. Clean the sentences by removing special characters.\n",
    "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
    "4. Pad each sentence to a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:26.710535Z",
     "iopub.status.busy": "2022-12-14T13:53:26.709875Z",
     "iopub.status.idle": "2022-12-14T13:53:26.852614Z",
     "shell.execute_reply": "2022-12-14T13:53:26.851762Z"
    },
    "id": "kRVATYOgJs1b"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "path_to_file = pathlib.Path('../data/gloss-text-augmented.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:26.856293Z",
     "iopub.status.busy": "2022-12-14T13:53:26.855695Z",
     "iopub.status.idle": "2022-12-14T13:53:26.860524Z",
     "shell.execute_reply": "2022-12-14T13:53:26.859728Z"
    },
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  text = path.read_text(encoding='utf-8')\n",
    "\n",
    "  lines = text.splitlines()\n",
    "  pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "  context = np.array([context.lower() for context,target in pairs])\n",
    "  target = np.array([target.lower() for context,target in pairs])\n",
    "\n",
    "  return target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:26.863774Z",
     "iopub.status.busy": "2022-12-14T13:53:26.863146Z",
     "iopub.status.idle": "2022-12-14T13:53:27.384149Z",
     "shell.execute_reply": "2022-12-14T13:53:27.383109Z"
    },
    "id": "cTbSbBz55QtF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what be xyou name ?\n"
     ]
    }
   ],
   "source": [
    "target_raw, context_raw = load_data(path_to_file)\n",
    "print(context_raw[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:27.388169Z",
     "iopub.status.busy": "2022-12-14T13:53:27.387490Z",
     "iopub.status.idle": "2022-12-14T13:53:27.391883Z",
     "shell.execute_reply": "2022-12-14T13:53:27.391094Z"
    },
    "id": "lH_dPY8TRp3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is your name ?\n"
     ]
    }
   ],
   "source": [
    "print(target_raw[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfVWx3WaI5Df"
   },
   "source": [
    "From these arrays of strings we can create a `tf.data.Dataset` of strings that shuffles and batches them efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:27.395550Z",
     "iopub.status.busy": "2022-12-14T13:53:27.394896Z",
     "iopub.status.idle": "2022-12-14T13:53:31.109819Z",
     "shell.execute_reply": "2022-12-14T13:53:31.109081Z"
    },
    "id": "3rZFgz69nMPa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 03:22:26.165184: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(context_raw)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw[is_train], target_raw[is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw[~is_train], target_raw[~is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:31.114119Z",
     "iopub.status.busy": "2022-12-14T13:53:31.113575Z",
     "iopub.status.idle": "2022-12-14T13:53:31.275545Z",
     "shell.execute_reply": "2022-12-14T13:53:31.274854Z"
    },
    "id": "qc6-NK1GtWQt",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'six month ceasefire , which expire on 19 december , be desc-far from desc-perfect .'\n",
      " b'what have change ?'\n",
      " b'x-i do desc-not believe that this be desc-right course .'\n",
      " b'desc-refore x-we should desc-not lose momentum on desc-rapidly reach desc-ambitious conclusion doha development round .'\n",
      " b'next item be council and commission statement on eu strategy for baltic sea area .'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'the six month ceasefire , which expired on 19 december , was far from perfect .'\n",
      " b'what has changed ?' b'i do not believe that this is the right course .'\n",
      " b'therefore we should not lose momentum on rapidly reaching an ambitious conclusion of the doha development round .'\n",
      " b'the next item is the council and commission statements on the eu strategy for the baltic sea area .'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_context_strings, example_target_strings in train_raw.take(1):\n",
    "  print(example_context_strings[:5])\n",
    "  print()\n",
    "  print(example_target_strings[:5])\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCoxLcuN3bwv"
   },
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kwdPcHvzz_a"
   },
   "source": [
    "One of the goals of this implementation is to build a model that can be exported as a `tf.saved_model`. To make that exported model useful it should take `tf.string` inputs, and return `tf.string` outputs: All the text processing happens inside the model. Mainly using a `layers.TextVectorization` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOQ5n55X4uDB"
   },
   "source": [
    "#### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:31.279684Z",
     "iopub.status.busy": "2022-12-14T13:53:31.279165Z",
     "iopub.status.idle": "2022-12-14T13:53:31.284225Z",
     "shell.execute_reply": "2022-12-14T13:53:31.283647Z"
    },
    "id": "mD0e-DWGQ2Vo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'this be why this report be desc-devoted desc-mainly to x-y .'\n",
      "b'this be why this report be desc-devoted desc-mainly to x-y .'\n"
     ]
    }
   ],
   "source": [
    "example_text = tf.constant('this be why this report be desc-devoted desc-mainly to x-y .')\n",
    "\n",
    "print(example_text.numpy())\n",
    "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hTllEjK6RSo"
   },
   "source": [
    "Unicode normalization will be the first step in the text standardization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:31.287728Z",
     "iopub.status.busy": "2022-12-14T13:53:31.287161Z",
     "iopub.status.idle": "2022-12-14T13:53:31.291699Z",
     "shell.execute_reply": "2022-12-14T13:53:31.291111Z"
    },
    "id": "chTF5N885F0P"
   },
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "  # Split accented characters.\n",
    "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "  text = tf.strings.lower(text)\n",
    "  # Keep space, a to z, and select punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "  # Add spaces around punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "  # Strip whitespace.\n",
    "  text = tf.strings.strip(text)\n",
    "\n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:31.295303Z",
     "iopub.status.busy": "2022-12-14T13:53:31.294695Z",
     "iopub.status.idle": "2022-12-14T13:53:31.303568Z",
     "shell.execute_reply": "2022-12-14T13:53:31.302985Z"
    },
    "id": "UREvDg3sEKYa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this be why this report be desc-devoted desc-mainly to x-y .\n",
      "[START] this be why this report be descdevoted descmainly to xy  . [END]\n"
     ]
    }
   ],
   "source": [
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4q-sKsSI7xRZ"
   },
   "source": [
    "#### Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aKn8qd37abi"
   },
   "source": [
    "This standardization function will be wrapped up in a `tf.keras.layers.TextVectorization` layer which will handle the vocabulary extraction and conversion of input text to sequences of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:31.306633Z",
     "iopub.status.busy": "2022-12-14T13:53:31.306387Z",
     "iopub.status.idle": "2022-12-14T13:53:31.318888Z",
     "shell.execute_reply": "2022-12-14T13:53:31.318172Z"
    },
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 5000\n",
    "\n",
    "context_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kbC6ODP8IK_"
   },
   "source": [
    "The `TextVectorization` layer and many other [Keras preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) have an `adapt` method. This method reads one epoch of the training data, and works a lot like `Model.fit`. This `adapt` method initializes the layer based on the data. Here it determines the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:31.322181Z",
     "iopub.status.busy": "2022-12-14T13:53:31.321622Z",
     "iopub.status.idle": "2022-12-14T13:53:33.629086Z",
     "shell.execute_reply": "2022-12-14T13:53:33.628165Z"
    },
    "id": "bmsI1Yql8FYe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/juicydoggo/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " '[START]',\n",
       " '[END]',\n",
       " '.',\n",
       " 'be',\n",
       " ',',\n",
       " 'to',\n",
       " 'xwe',\n",
       " 'in',\n",
       " 'and',\n",
       " 'this',\n",
       " 'that',\n",
       " 'xi',\n",
       " 'have',\n",
       " 'for',\n",
       " 'xit',\n",
       " 'descnot',\n",
       " 'on',\n",
       " 'will']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
    "\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "context_text_processor.get_vocabulary()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kGjIFjX8_Wp"
   },
   "source": [
    "That's the Gloss `TextVectorization` layer, now build and `.adapt()` the Text one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:33.633770Z",
     "iopub.status.busy": "2022-12-14T13:53:33.633065Z",
     "iopub.status.idle": "2022-12-14T13:53:35.836766Z",
     "shell.execute_reply": "2022-12-14T13:53:35.835935Z"
    },
    "id": "jlC4xuZnKLBS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " '[START]',\n",
       " '[END]',\n",
       " '.',\n",
       " 'the',\n",
       " ',',\n",
       " 'of',\n",
       " 'to',\n",
       " 'is',\n",
       " 'in',\n",
       " 'and',\n",
       " 'this',\n",
       " 'a',\n",
       " 'we',\n",
       " 'that',\n",
       " 'i',\n",
       " 'for',\n",
       " 'it',\n",
       " 'be']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True)\n",
    "\n",
    "target_text_processor.adapt(train_raw.map(lambda context, target: target))\n",
    "target_text_processor.get_vocabulary()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWQqlP_s9eIv"
   },
   "source": [
    "Now these layers can convert a batch of strings into a batch of token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:35.840526Z",
     "iopub.status.busy": "2022-12-14T13:53:35.839872Z",
     "iopub.status.idle": "2022-12-14T13:53:35.889422Z",
     "shell.execute_reply": "2022-12-14T13:53:35.888738Z"
    },
    "id": "9KZxj8IrNZ9S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[2, 826, 476, 1971, 6, 50, 3042, 18, 910, 6, 5, 327, 49, 2137, 4, 3],\n",
       " [2, 46, 14, 111, 28, 3], [2, 13, 23, 17, 110, 12, 11, 5, 141, 169, 4, 3]]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokens = context_text_processor(example_context_strings)\n",
    "example_tokens[:3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA9rUn9G9n78"
   },
   "source": [
    "The `get_vocabulary` method can be used to convert token IDs back to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:35.893034Z",
     "iopub.status.busy": "2022-12-14T13:53:35.892407Z",
     "iopub.status.idle": "2022-12-14T13:53:35.909454Z",
     "shell.execute_reply": "2022-12-14T13:53:35.908852Z"
    },
    "id": "98g9rcxGQY0I"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START] six month ceasefire , which expire on december , be descfar from descperfect . [END]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
    "tokens = context_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ot0aCL9t-Ghi"
   },
   "source": [
    "The returned token IDs are zero-padded. This can easily be turned into a mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:35.912742Z",
     "iopub.status.busy": "2022-12-14T13:53:35.912188Z",
     "iopub.status.idle": "2022-12-14T13:53:36.157685Z",
     "shell.execute_reply": "2022-12-14T13:53:36.156965Z"
    },
    "id": "_jx4Or_eFRSz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mask')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(example_tokens.to_tensor())\n",
    "plt.title('Token IDs')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(example_tokens.to_tensor() != 0)\n",
    "plt.title('Mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3O0B4XdFlRgc"
   },
   "source": [
    "### Process the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVCuyuSp_whd"
   },
   "source": [
    "The `process_text` function below converts the `Datasets` of strings, into  0-padded tensors of token IDs. It also converts from a `(context, target)` pair to an `((context, target_in), target_out)` pair for training with `keras.Model.fit`. Keras expects `(inputs, labels)` pairs, the inputs are the `(context, target_in)` and the labels are `target_out`. The difference between `target_in` and `target_out` is that they are shifted by one step relative to eachother, so that at each location the label is the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:36.161705Z",
     "iopub.status.busy": "2022-12-14T13:53:36.161123Z",
     "iopub.status.idle": "2022-12-14T13:53:36.376562Z",
     "shell.execute_reply": "2022-12-14T13:53:36.375836Z"
    },
    "id": "wk5tbZWQl5u1"
   },
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "  context = context_text_processor(context).to_tensor()\n",
    "  target = target_text_processor(target)\n",
    "  targ_in = target[:,:-1].to_tensor()\n",
    "  targ_out = target[:,1:].to_tensor()\n",
    "  return (context, targ_in), targ_out\n",
    "\n",
    "\n",
    "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
    "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iGi7X2m_tbM"
   },
   "source": [
    "Here is the first sequence of each, from the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:36.380626Z",
     "iopub.status.busy": "2022-12-14T13:53:36.380088Z",
     "iopub.status.idle": "2022-12-14T13:53:36.595128Z",
     "shell.execute_reply": "2022-12-14T13:53:36.594462Z"
    },
    "id": "woQBWAjLsJkr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  48  14 523   7  47 109 811 552  93]\n",
      "\n",
      "[   2    5   55   27 2339    8   85  108   13 1020]\n",
      "[   5   55   27 2339    8   85  108   13 1020 1270]\n"
     ]
    }
   ],
   "source": [
    "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
    "  print(ex_context_tok[0, :10].numpy()) \n",
    "  print()\n",
    "  print(ex_tar_in[0, :10].numpy()) \n",
    "  print(ex_tar_out[0, :10].numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNfHIF71ulLu"
   },
   "source": [
    "## The encoder/decoder\n",
    "\n",
    "The following diagrams shows an overview of the model. In both the encoder is on the left, the decoder is on the right. At each time-step the decoder's output is combined with the encoder's output, to predict the next word. \n",
    "\n",
    "The original [left] contains a few extra connections that are intentionally omitted from this model [right], as they are generally unnecessary, and difficult to implement. Those missing connections are:\n",
    "\n",
    "1. Feeding the state from the encoder's RNN to the decoder's RNN\n",
    "2. Feeding the attention output back to the RNN's input.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=500 src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\"/>\n",
    "  </td>\n",
    "  <td>\n",
    "   <img width=380 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN+attention.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <th colspan=1>The original from <a href=https://arxiv.org/abs/1508.04025v5>Effective Approaches to Attention-based Neural Machine Translation</a></th>\n",
    "  <th colspan=1></th>\n",
    "<tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzQWx2saImMV"
   },
   "source": [
    "Before getting into it define constants for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:36.599307Z",
     "iopub.status.busy": "2022-12-14T13:53:36.598631Z",
     "iopub.status.idle": "2022-12-14T13:53:36.602039Z",
     "shell.execute_reply": "2022-12-14T13:53:36.601401Z"
    },
    "id": "_a9uNz3-IrF-"
   },
   "outputs": [],
   "source": [
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blNgVbLSzpsr"
   },
   "source": [
    "### The encoder\n",
    "\n",
    "The goal of the encoder is to process the context sequence into a sequence of vectors that are useful for the decoder as it attempts to predict the next output for each timestep. Since the context sequence is constant, there is no restriction on how information can flow in the encoder, so use a bidirectional-RNN to do the processing:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=500 src=\"https://tensorflow.org/images/tutorials/transformer/RNN-bidirectional.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <th>A bidirectional RNN</th>\n",
    "<tr>\n",
    "</table>\n",
    "\n",
    "The encoder:\n",
    "\n",
    "1. Takes a list of token IDs (from `context_text_processor`).\n",
    "3. Looks up an embedding vector for each token (Using a `layers.Embedding`).\n",
    "4. Processes the embeddings into a new sequence (Using a bidirectional `layers.GRU`).\n",
    "5. Returns the processed sequence. This will be passed to the attention head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:36.605407Z",
     "iopub.status.busy": "2022-12-14T13:53:36.604915Z",
     "iopub.status.idle": "2022-12-14T13:53:36.611486Z",
     "shell.execute_reply": "2022-12-14T13:53:36.610923Z"
    },
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, text_processor, units):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.text_processor = text_processor\n",
    "    self.vocab_size = text_processor.vocabulary_size()\n",
    "    self.units = units\n",
    "    \n",
    "    # The embedding layer converts tokens to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n",
    "                                               mask_zero=True)\n",
    "\n",
    "    # The RNN layer processes those vectors sequentially.\n",
    "    self.rnn = tf.keras.layers.Bidirectional(\n",
    "        merge_mode='sum',\n",
    "        layer=tf.keras.layers.GRU(units,\n",
    "                            # Return the sequence and state\n",
    "                            return_sequences=True,\n",
    "                            recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "  def call(self, x):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(x, 'batch s')\n",
    "\n",
    "    # 2. The embedding layer looks up the embedding vector for each token.\n",
    "    x = self.embedding(x)\n",
    "    shape_checker(x, 'batch s units')\n",
    "\n",
    "    # 3. The GRU processes the sequence of embeddings.\n",
    "    x = self.rnn(x)\n",
    "    shape_checker(x, 'batch s units')\n",
    "\n",
    "    # 4. Returns the new sequence of embeddings.\n",
    "    return x\n",
    "\n",
    "  def convert_input(self, texts):\n",
    "    texts = tf.convert_to_tensor(texts)\n",
    "    if len(texts.shape) == 0:\n",
    "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
    "    context = self.text_processor(texts).to_tensor()\n",
    "    context = self(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:36.615068Z",
     "iopub.status.busy": "2022-12-14T13:53:36.614428Z",
     "iopub.status.idle": "2022-12-14T13:53:37.303226Z",
     "shell.execute_reply": "2022-12-14T13:53:37.302546Z"
    },
    "id": "60gSVh05Jl6l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens, shape (batch, s): (64, 21)\n",
      "Encoder output, shape (batch, s, units): (64, 21, 256)\n"
     ]
    }
   ],
   "source": [
    "# Encode the input sequence.\n",
    "encoder = Encoder(context_text_processor, UNITS)\n",
    "ex_context = encoder(ex_context_tok)\n",
    "\n",
    "print(f'Context tokens, shape (batch, s): {ex_context_tok.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45xM_Gl1MgXY"
   },
   "source": [
    "### The attention layer\n",
    "\n",
    "The attention layer lets the decoder access the information extracted by the encoder. It computes a vector from the entire context sequence, and adds that to the decoder's output. \n",
    "\n",
    "The simplest way you could calculate a single vector from the entire sequence would be to take the average across the sequence (`layers.GlobalAveragePooling1D`). An attention layer is similar, but calculates a **weighted** average across the context sequence. Where the weights are calculated from the combination of context and \"query\" vectors.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention-new-full.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <th colspan=1>The attention layer</th>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:37.307080Z",
     "iopub.status.busy": "2022-12-14T13:53:37.306521Z",
     "iopub.status.idle": "2022-12-14T13:53:37.312683Z",
     "shell.execute_reply": "2022-12-14T13:53:37.312023Z"
    },
    "id": "-Ql3ymqwD8LS"
   },
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "  def call(self, x, context):\n",
    "    shape_checker = ShapeChecker()\n",
    " \n",
    "    shape_checker(x, 'batch t units')\n",
    "    shape_checker(context, 'batch s units')\n",
    "\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "    \n",
    "    shape_checker(x, 'batch t units')\n",
    "    shape_checker(attn_scores, 'batch heads t s')\n",
    "    \n",
    "    # Cache the attention scores for plotting later.\n",
    "    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
    "    shape_checker(attn_scores, 'batch t s')\n",
    "    self.last_attention_weights = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:37.315939Z",
     "iopub.status.busy": "2022-12-14T13:53:37.315467Z",
     "iopub.status.idle": "2022-12-14T13:53:37.468755Z",
     "shell.execute_reply": "2022-12-14T13:53:37.468036Z"
    },
    "id": "7y7hjPkNMmHh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context sequence, shape (batch, s, units): (64, 21, 256)\n",
      "Target sequence, shape (batch, t, units): (64, 23, 256)\n",
      "Attention result, shape (batch, t, units): (64, 23, 256)\n",
      "Attention weights, shape (batch, t, s):    (64, 23, 21)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = CrossAttention(UNITS)\n",
    "\n",
    "# Attend to the encoded tokens\n",
    "embed = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(),\n",
    "                                  output_dim=UNITS, mask_zero=True)\n",
    "ex_tar_embed = embed(ex_tar_in)\n",
    "\n",
    "result = attention_layer(ex_tar_embed, ex_context)\n",
    "\n",
    "print(f'Context sequence, shape (batch, s, units): {ex_context.shape}')\n",
    "print(f'Target sequence, shape (batch, t, units): {ex_tar_embed.shape}')\n",
    "print(f'Attention result, shape (batch, t, units): {result.shape}')\n",
    "print(f'Attention weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vx9fUhi3Pmwp"
   },
   "source": [
    "The attention weights will sum to `1` over the context sequence, at each location in the target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:37.472854Z",
     "iopub.status.busy": "2022-12-14T13:53:37.472230Z",
     "iopub.status.idle": "2022-12-14T13:53:37.478239Z",
     "shell.execute_reply": "2022-12-14T13:53:37.477607Z"
    },
    "id": "zxyR7cmQPn9P"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , 1.        , 0.9999999 ,\n",
       "       1.        , 1.        , 0.99999994, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.0000001 , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        ], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer.last_attention_weights[0].numpy().sum(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AagyXMH-Jhqt"
   },
   "source": [
    "\n",
    "\n",
    "Here are the attention weights across the context sequences at `t=0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:37.481565Z",
     "iopub.status.busy": "2022-12-14T13:53:37.481145Z",
     "iopub.status.idle": "2022-12-14T13:53:37.715882Z",
     "shell.execute_reply": "2022-12-14T13:53:37.715248Z"
    },
    "id": "Rqr8XGsAJlf6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_weights = attention_layer.last_attention_weights\n",
    "mask=(ex_context_tok != 0).numpy()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(mask*attention_weights[:, 0, :])\n",
    "plt.title('Attention weights')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(mask)\n",
    "plt.title('Mask');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Eil-C_NN1rp"
   },
   "source": [
    "Because of the small-random initialization the attention weights are initially all close to `1/(sequence_length)`. The model will learn to make these less uniform as training progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQ638eHN4iCK"
   },
   "source": [
    "### The decoder\n",
    "\n",
    "The decoder's job is to generate predictions for the next token at each location in the target sequence.\n",
    "\n",
    "1. It looks up embeddings for each token in the target sequence.\n",
    "2. It uses an RNN to process the target sequence, and keep track of what it has generated so far.\n",
    "3. It uses RNN output as the \"query\" to the attention layer, when attending to the encoder's output.\n",
    "4. At each location in the output it predicts the next token.\n",
    "\n",
    "When training, the model predicts the next word at each location. So it's important that the information only flows in one direction through the model. The decoder uses a unidirectional (not bidirectional) RNN to process the target sequence.\n",
    "\n",
    "When running inference with this model it produces one word at a time, and those are fed back into the model.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=500 src=\"https://tensorflow.org/images/tutorials/transformer/RNN.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <th>A unidirectional RNN</th>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZsQJMqNmg_L"
   },
   "source": [
    "Here is the `Decoder` class' initializer. The initializer creates all the necessary layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:37.719773Z",
     "iopub.status.busy": "2022-12-14T13:53:37.719277Z",
     "iopub.status.idle": "2022-12-14T13:53:37.725677Z",
     "shell.execute_reply": "2022-12-14T13:53:37.725101Z"
    },
    "id": "erYvHIgAl8kh"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, text_processor, units):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.text_processor = text_processor\n",
    "    self.vocab_size = text_processor.vocabulary_size()\n",
    "    self.word_to_id = tf.keras.layers.StringLookup(\n",
    "        vocabulary=text_processor.get_vocabulary(),\n",
    "        mask_token='', oov_token='[UNK]')\n",
    "    self.id_to_word = tf.keras.layers.StringLookup(\n",
    "        vocabulary=text_processor.get_vocabulary(),\n",
    "        mask_token='', oov_token='[UNK]',\n",
    "        invert=True)\n",
    "    self.start_token = self.word_to_id('[START]')\n",
    "    self.end_token = self.word_to_id('[END]')\n",
    "\n",
    "    self.units = units\n",
    "\n",
    "\n",
    "    # 1. The embedding layer converts token IDs to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
    "                                               units, mask_zero=True)\n",
    "\n",
    "    # 2. The RNN keeps track of what's been generated so far.\n",
    "    self.rnn = tf.keras.layers.GRU(units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    # 3. The RNN output will be the query for the attention layer.\n",
    "    self.attention = CrossAttention(units)\n",
    "\n",
    "    # 4. This fully connected layer produces the logits for each\n",
    "    # output token.\n",
    "    self.output_layer = tf.keras.layers.Dense(self.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sd8-nRNzFR8x"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPnaw583CpnY"
   },
   "source": [
    "Next, the `call` method, takes 3 arguments:\n",
    "\n",
    "* `inputs` -  a `context, x` pair where:\n",
    "  * `context` - is the context from the encoder's output.\n",
    "  * `x` - is the target sequence input.\n",
    "* `state` - Optional, the previous `state` output from the decoder (the internal state of the decoder's RNN). Pass the state from a previous run to continue generating text where you left off.\n",
    "* `return_state` - [Default: False] - Set this to `True` to return the RNN state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:37.729209Z",
     "iopub.status.busy": "2022-12-14T13:53:37.728744Z",
     "iopub.status.idle": "2022-12-14T13:53:37.733952Z",
     "shell.execute_reply": "2022-12-14T13:53:37.733371Z"
    },
    "id": "PJOi5btHAPNK"
   },
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def call(self,\n",
    "         context, x,\n",
    "         state=None,\n",
    "         return_state=False):  \n",
    "  shape_checker = ShapeChecker()\n",
    "  shape_checker(x, 'batch t')\n",
    "  shape_checker(context, 'batch s units')\n",
    "\n",
    "  # 1. Lookup the embeddings\n",
    "  x = self.embedding(x)\n",
    "  shape_checker(x, 'batch t units')\n",
    "\n",
    "  # 2. Process the target sequence.\n",
    "  x, state = self.rnn(x, initial_state=state)\n",
    "  shape_checker(x, 'batch t units')\n",
    "\n",
    "  # 3. Use the RNN output as the query for the attention over the context.\n",
    "  x = self.attention(x, context)\n",
    "  self.last_attention_weights = self.attention.last_attention_weights\n",
    "  shape_checker(x, 'batch t units')\n",
    "  shape_checker(self.last_attention_weights, 'batch t s')\n",
    "\n",
    "  # Step 4. Generate logit predictions for the next token.\n",
    "  logits = self.output_layer(x)\n",
    "  shape_checker(logits, 'batch t target_vocab_size')\n",
    "\n",
    "  if return_state:\n",
    "    return logits, state\n",
    "  else:\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:37.737178Z",
     "iopub.status.busy": "2022-12-14T13:53:37.736732Z",
     "iopub.status.idle": "2022-12-14T13:53:37.832773Z",
     "shell.execute_reply": "2022-12-14T13:53:37.832171Z"
    },
    "id": "4ZUMbYXIEVeA"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(target_text_processor, UNITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFWaI4wqzt4t"
   },
   "source": [
    "In training the decoder will behave like this:\n",
    "\n",
    "Given the context and target tokens, for each target token it predicts the next target token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:37.836650Z",
     "iopub.status.busy": "2022-12-14T13:53:37.835861Z",
     "iopub.status.idle": "2022-12-14T13:53:37.896769Z",
     "shell.execute_reply": "2022-12-14T13:53:37.896077Z"
    },
    "id": "5YM-lD7bzx18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder output shape: (batch, s, units) (64, 21, 256)\n",
      "input target tokens shape: (batch, t) (64, 23)\n",
      "logits shape shape: (batch, target_vocabulary_size) (64, 23, 5000)\n"
     ]
    }
   ],
   "source": [
    "logits = decoder(ex_context, ex_tar_in)\n",
    "\n",
    "print(f'encoder output shape: (batch, s, units) {ex_context.shape}')\n",
    "print(f'input target tokens shape: (batch, t) {ex_tar_in.shape}')\n",
    "print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhS_tbk7VQkX"
   },
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:37.900171Z",
     "iopub.status.busy": "2022-12-14T13:53:37.899703Z",
     "iopub.status.idle": "2022-12-14T13:53:37.903500Z",
     "shell.execute_reply": "2022-12-14T13:53:37.902953Z"
    },
    "id": "SPm12cnIVRQr"
   },
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_initial_state(self, context):\n",
    "  batch_size = tf.shape(context)[0]\n",
    "  start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "  embedded = self.embedding(start_tokens)\n",
    "  return start_tokens, done, self.rnn.get_initial_state(embedded)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:37.906633Z",
     "iopub.status.busy": "2022-12-14T13:53:37.906194Z",
     "iopub.status.idle": "2022-12-14T13:53:37.910082Z",
     "shell.execute_reply": "2022-12-14T13:53:37.909439Z"
    },
    "id": "TzeOhpBvVS5L"
   },
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def tokens_to_text(self, tokens):\n",
    "  words = self.id_to_word(tokens)\n",
    "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "  result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
    "  result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:37.913203Z",
     "iopub.status.busy": "2022-12-14T13:53:37.912770Z",
     "iopub.status.idle": "2022-12-14T13:53:37.917262Z",
     "shell.execute_reply": "2022-12-14T13:53:37.916636Z"
    },
    "id": "v6ildnz_V1MA"
   },
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
    "  logits, state = self(\n",
    "    context, next_token,\n",
    "    state = state,\n",
    "    return_state=True) \n",
    "  \n",
    "  if temperature == 0.0:\n",
    "    next_token = tf.argmax(logits, axis=-1)\n",
    "  else:\n",
    "    logits = logits[:, -1, :]/temperature\n",
    "    next_token = tf.random.categorical(logits, num_samples=1)\n",
    "\n",
    "  # If a sequence produces an `end_token`, set it `done`\n",
    "  done = done | (next_token == self.end_token)\n",
    "  # Once a sequence is done it only produces 0-padding.\n",
    "  next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
    "  \n",
    "  return next_token, done, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:37.920422Z",
     "iopub.status.busy": "2022-12-14T13:53:37.920046Z",
     "iopub.status.idle": "2022-12-14T13:53:38.166596Z",
     "shell.execute_reply": "2022-12-14T13:53:38.165918Z"
    },
    "id": "SuehagxL-JBZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'businesses option harassment next problem statements airports alzheimers availability stringent',\n",
       "       b'reliable storage signature slavery therefore thinks problematic delay hopefully criticism',\n",
       "       b'located populations coordinating positions regard started accepted works [UNK] websites'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the loop variables.\n",
    "next_token, done, state = decoder.get_initial_state(ex_context)\n",
    "tokens = []\n",
    "\n",
    "for n in range(10):\n",
    "  # Run one step.\n",
    "  next_token, done, state = decoder.get_next_token(\n",
    "      ex_context, next_token, done, state, temperature=1.0)\n",
    "  # Add the token to the output.\n",
    "  tokens.append(next_token)\n",
    "\n",
    "# Stack all the tokens together.\n",
    "tokens = tf.concat(tokens, axis=-1) # (batch, t)\n",
    "\n",
    "# Convert the tokens back to a a string\n",
    "result = decoder.tokens_to_text(tokens)\n",
    "result[:3].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ALTdqCMLGSY"
   },
   "source": [
    "Since the model's untrained, it outputs items from the vocabulary almost uniformly at random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6xyru86m914"
   },
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:38.170581Z",
     "iopub.status.busy": "2022-12-14T13:53:38.170022Z",
     "iopub.status.idle": "2022-12-14T13:53:38.175086Z",
     "shell.execute_reply": "2022-12-14T13:53:38.174388Z"
    },
    "id": "WWIyuy71TkJT"
   },
   "outputs": [],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, units,\n",
    "               context_text_processor,\n",
    "               target_text_processor):\n",
    "    super().__init__()\n",
    "    # Build the encoder and decoder\n",
    "    encoder = Encoder(context_text_processor, units)\n",
    "    decoder = Decoder(target_text_processor, units)\n",
    "\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def call(self, inputs):\n",
    "    context, x = inputs\n",
    "    context = self.encoder(context)\n",
    "    logits = self.decoder(context, x)\n",
    "    try:\n",
    "      # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rPi0FkS2iA5"
   },
   "source": [
    "During training the model will be used like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:38.178812Z",
     "iopub.status.busy": "2022-12-14T13:53:38.178363Z",
     "iopub.status.idle": "2022-12-14T13:53:38.359380Z",
     "shell.execute_reply": "2022-12-14T13:53:38.358700Z"
    },
    "id": "8vhjTh84K6Mg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens, shape: (batch, s, units) (64, 21)\n",
      "Target tokens, shape: (batch, t) (64, 23)\n",
      "logits, shape: (batch, t, target_vocabulary_size) (64, 23, 5000)\n"
     ]
    }
   ],
   "source": [
    "model = Translator(UNITS, context_text_processor, target_text_processor)\n",
    "\n",
    "logits = model((ex_context_tok, ex_tar_in))\n",
    "\n",
    "print(f'Context tokens, shape: (batch, s, units) {ex_context_tok.shape}')\n",
    "print(f'Target tokens, shape: (batch, t) {ex_tar_in.shape}')\n",
    "print(f'logits, shape: (batch, t, target_vocabulary_size) {logits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:38.362806Z",
     "iopub.status.busy": "2022-12-14T13:53:38.362542Z",
     "iopub.status.idle": "2022-12-14T13:53:38.366866Z",
     "shell.execute_reply": "2022-12-14T13:53:38.366216Z"
    },
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:38.370186Z",
     "iopub.status.busy": "2022-12-14T13:53:38.369665Z",
     "iopub.status.idle": "2022-12-14T13:53:38.373568Z",
     "shell.execute_reply": "2022-12-14T13:53:38.372969Z"
    },
    "id": "nRB1CTmQWOIL"
   },
   "outputs": [],
   "source": [
    "def masked_acc(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "    \n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    \n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f32GuAhw2nXm"
   },
   "source": [
    "Configure the model for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:38.376951Z",
     "iopub.status.busy": "2022-12-14T13:53:38.376399Z",
     "iopub.status.idle": "2022-12-14T13:53:38.405030Z",
     "shell.execute_reply": "2022-12-14T13:53:38.404311Z"
    },
    "id": "9g0DRRvm3l9X"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=masked_loss, \n",
    "              metrics=[masked_acc, masked_loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DWLI3pssjnx"
   },
   "source": [
    "The model is randomly initialized, and should give roughly uniform output probabilities. So it's easy to predict what the initial values of the metrics should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:38.408947Z",
     "iopub.status.busy": "2022-12-14T13:53:38.408411Z",
     "iopub.status.idle": "2022-12-14T13:53:38.414979Z",
     "shell.execute_reply": "2022-12-14T13:53:38.414386Z"
    },
    "id": "BuP3_LFENMJG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'expected_loss': 8.517193, 'expected_acc': 0.0002}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 1.0 * target_text_processor.vocabulary_size()\n",
    "\n",
    "{\"expected_loss\": tf.math.log(vocab_size).numpy(),\n",
    " \"expected_acc\": 1/vocab_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frVba49Usd0Z"
   },
   "source": [
    "That should roughly match the values returned by running a few steps of evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:38.418259Z",
     "iopub.status.busy": "2022-12-14T13:53:38.417876Z",
     "iopub.status.idle": "2022-12-14T13:53:45.431110Z",
     "shell.execute_reply": "2022-12-14T13:53:45.430452Z"
    },
    "id": "8rJITfxEsHKR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 11s 169ms/step - loss: 8.5254 - masked_acc: 2.8584e-04 - masked_loss: 8.5254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 8.525419235229492,\n",
       " 'masked_acc': 0.0002858423686120659,\n",
       " 'masked_loss': 8.525419235229492}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_ds, steps=20, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:53:45.434651Z",
     "iopub.status.busy": "2022-12-14T13:53:45.433998Z",
     "iopub.status.idle": "2022-12-14T13:55:54.435494Z",
     "shell.execute_reply": "2022-12-14T13:55:54.434762Z"
    },
    "id": "BQd_esVVoSf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 45s 358ms/step - loss: 5.4997 - masked_acc: 0.1955 - masked_loss: 5.4997 - val_loss: 4.4608 - val_masked_acc: 0.3184 - val_masked_loss: 4.4608\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 45s 451ms/step - loss: 3.8844 - masked_acc: 0.4040 - masked_loss: 3.8844 - val_loss: 3.1898 - val_masked_acc: 0.5087 - val_masked_loss: 3.1898\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 41s 412ms/step - loss: 2.4499 - masked_acc: 0.6118 - masked_loss: 2.4499 - val_loss: 1.9495 - val_masked_acc: 0.6642 - val_masked_loss: 1.9495\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 42s 415ms/step - loss: 1.5711 - masked_acc: 0.7251 - masked_loss: 1.5711 - val_loss: 1.1638 - val_masked_acc: 0.7765 - val_masked_loss: 1.1638\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 41s 409ms/step - loss: 1.0089 - masked_acc: 0.8023 - masked_loss: 1.0089 - val_loss: 0.8395 - val_masked_acc: 0.8245 - val_masked_loss: 0.8395\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 41s 415ms/step - loss: 0.8171 - masked_acc: 0.8242 - masked_loss: 0.8171 - val_loss: 0.9751 - val_masked_acc: 0.7929 - val_masked_loss: 0.9751\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 45s 455ms/step - loss: 0.7713 - masked_acc: 0.8263 - masked_loss: 0.7713 - val_loss: 0.6106 - val_masked_acc: 0.8498 - val_masked_loss: 0.6106\n",
      "Epoch 8/100\n",
      " 42/100 [===========>..................] - ETA: 22s - loss: 0.5919 - masked_acc: 0.8539 - masked_loss: 0.5919"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds.repeat(), \n",
    "    epochs=100,\n",
    "    steps_per_epoch = 100,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps = 20,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:55:54.439314Z",
     "iopub.status.busy": "2022-12-14T13:55:54.438809Z",
     "iopub.status.idle": "2022-12-14T13:55:54.597838Z",
     "shell.execute_reply": "2022-12-14T13:55:54.596991Z"
    },
    "id": "38rLdlmtQHCm"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:55:54.601744Z",
     "iopub.status.busy": "2022-12-14T13:55:54.601260Z",
     "iopub.status.idle": "2022-12-14T13:55:54.758966Z",
     "shell.execute_reply": "2022-12-14T13:55:54.758112Z"
    },
    "id": "KkhXRASNG80_"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['masked_acc'], label='accuracy')\n",
    "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "### Translate\n",
    "\n",
    "Now that the model is trained, implement a function to execute the full `text => text` translation. This code is basically identical to the [inference example](#inference) in the [decoder section](#the_decoder), but this also captures the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:55:54.762645Z",
     "iopub.status.busy": "2022-12-14T13:55:54.762120Z",
     "iopub.status.idle": "2022-12-14T13:55:54.768161Z",
     "shell.execute_reply": "2022-12-14T13:55:54.767292Z"
    },
    "id": "mmgYPCVgEwp_"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "@Translator.add_method\n",
    "def translate(self,\n",
    "              texts, *,\n",
    "              max_length=50,\n",
    "              temperature=0.0):\n",
    "  # Process the input texts\n",
    "  context = self.encoder.convert_input(texts)\n",
    "  batch_size = tf.shape(texts)[0]\n",
    "\n",
    "  # Setup the loop inputs\n",
    "  tokens = []\n",
    "  attention_weights = []\n",
    "  next_token, done, state = self.decoder.get_initial_state(context)\n",
    "\n",
    "  for _ in range(max_length):\n",
    "    # Generate the next token\n",
    "    next_token, done, state = self.decoder.get_next_token(\n",
    "        context, next_token, done,  state, temperature)\n",
    "        \n",
    "    # Collect the generated tokens\n",
    "    tokens.append(next_token)\n",
    "    attention_weights.append(self.decoder.last_attention_weights)\n",
    "    \n",
    "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "      break\n",
    "\n",
    "  # Stack the lists of tokens and attention weights.\n",
    "  tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
    "  self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
    "\n",
    "  result = self.decoder.tokens_to_text(tokens)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:55:54.771943Z",
     "iopub.status.busy": "2022-12-14T13:55:54.771432Z",
     "iopub.status.idle": "2022-12-14T13:55:54.949359Z",
     "shell.execute_reply": "2022-12-14T13:55:54.948750Z"
    },
    "id": "E5hqvbR5FUCD"
   },
   "outputs": [],
   "source": [
    "result = model.translate(['this be why this report be desc-devoted desc-mainly to x-y .']) \n",
    "result[0].numpy().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ1iU63cVgfs"
   },
   "source": [
    "Use that to generate the attention plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:55:54.952758Z",
     "iopub.status.busy": "2022-12-14T13:55:54.952234Z",
     "iopub.status.idle": "2022-12-14T13:55:54.958423Z",
     "shell.execute_reply": "2022-12-14T13:55:54.957753Z"
    },
    "id": "s5hQWlbN3jGF"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "@Translator.add_method\n",
    "def plot_attention(self, text, **kwargs):\n",
    "  assert isinstance(text, str)\n",
    "  output = self.translate([text], **kwargs)\n",
    "  output = output[0].numpy().decode()\n",
    "\n",
    "  attention = self.last_attention_weights[0]\n",
    "\n",
    "  context = tf_lower_and_split_punct(text)\n",
    "  context = context.numpy().decode().split()\n",
    "\n",
    "  output = tf_lower_and_split_punct(output)\n",
    "  output = output.numpy().decode().split()[1:]\n",
    "\n",
    "  fig = plt.figure(figsize=(10, 10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + context, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + output, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  ax.set_xlabel('Input text')\n",
    "  ax.set_ylabel('Output text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:55:54.961723Z",
     "iopub.status.busy": "2022-12-14T13:55:54.961266Z",
     "iopub.status.idle": "2022-12-14T13:55:55.368749Z",
     "shell.execute_reply": "2022-12-14T13:55:55.367953Z"
    },
    "id": "rrGawQv2eiA4"
   },
   "outputs": [],
   "source": [
    "model.plot_attention('this be why this report be desc-devoted desc-mainly to x-y .') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHBdOf9duumm"
   },
   "source": [
    "Translate a few more sentences and plot them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rA3xI3NzrRJt"
   },
   "source": [
    "The short sentences often work well, but if the input is too long the model literally loses focus and stops providing reasonable predictions. There are two main reasons for this:\n",
    "\n",
    "1. The model was trained with teacher-forcing feeding the correct token at each step, regardless of the model's predictions. The model could be made more robust if it were sometimes fed its own predictions.\n",
    "2. The model only has access to its previous output through the RNN state. If the RNN state looses track of where it was in the context sequence there's no way for the model to recover. [Transformers](transformer.ipynb) improve on this by letting the decoder look at what it has output so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4POAuUgLxLv"
   },
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:55:58.079687Z",
     "iopub.status.busy": "2022-12-14T13:55:58.078988Z",
     "iopub.status.idle": "2022-12-14T13:55:58.083501Z",
     "shell.execute_reply": "2022-12-14T13:55:58.082908Z"
    },
    "id": "fNhGwQaVKIAy"
   },
   "outputs": [],
   "source": [
    "class Export(tf.Module):\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
    "  def translate(self, inputs):\n",
    "    return self.model.translate(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:55:58.086412Z",
     "iopub.status.busy": "2022-12-14T13:55:58.086180Z",
     "iopub.status.idle": "2022-12-14T13:55:58.089508Z",
     "shell.execute_reply": "2022-12-14T13:55:58.088833Z"
    },
    "id": "5Tjqs9FzNwW5"
   },
   "outputs": [],
   "source": [
    "export = Export(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:56:58.492688Z",
     "iopub.status.busy": "2022-12-14T13:56:58.492102Z",
     "iopub.status.idle": "2022-12-14T13:58:20.957361Z",
     "shell.execute_reply": "2022-12-14T13:58:20.956463Z"
    },
    "id": "OyvxT5V0_X5B"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.saved_model.save(export, 'translator-augmented',\n",
    "                    signatures={'serving_default': export.translate})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:58:20.963150Z",
     "iopub.status.busy": "2022-12-14T13:58:20.962332Z",
     "iopub.status.idle": "2022-12-14T13:59:15.828295Z",
     "shell.execute_reply": "2022-12-14T13:59:15.827578Z"
    },
    "id": "-I0j3i3ekOba"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# model = tf.saved_model.load('translator-augmented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "metric = datasets.load_metric('sacrebleu')\n",
    "print(metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the accuracy on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_input=context_raw[~is_train].tolist()\n",
    "valid_groundtruth=target_raw[~is_train].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_input=[i.lower() for i in valid_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "model_predictions=[]\n",
    "predict=model.translate(valid_input)\n",
    "for i in tqdm(range(len(predict))):\n",
    "    model_predictions.append(predict[i].numpy().decode())\n",
    "    valid_groundtruth[i]=[valid_groundtruth[i]]\n",
    "metric.add_batch(predictions=model_predictions, references=valid_groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(context_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[\n",
    "    'COUNTRY STRATEGY PAPERS MALAYSIUM , BRAZIL AND PAKISTAN VOTE',\n",
    "    'EC RUSSIUM DESC-SHORT STAY VISA AGREEMENT VOTE',\n",
    "    'EIB DESC-ANNUAL REPORT 2005 VOTE'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groundtruth\n",
    "- country strategy papers malaysia , brazil and pakistan vote \n",
    "- ec russia short stay visa agreement vote \n",
    "- eib annual report 2005 vote "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = export.translate(tf.constant(inputs))\n",
    "\n",
    "print(result[0].numpy().decode())\n",
    "print(result[1].numpy().decode())\n",
    "print(result[2].numpy().decode())\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "nmt_with_attention.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

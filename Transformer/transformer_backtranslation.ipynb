{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy1nIZPmL6sq"
      },
      "source": [
        "# Install "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtkaMmL7RaX_"
      },
      "outputs": [],
      "source": [
        "! pip install transformers datasets evaluate\n",
        "! pip install sacrebleu\n",
        "! pip install googletrans==3.1.0a0\n",
        "! pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VePYZpVkL6sw"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8dF5LznlL6sw"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import pipeline\n",
        "from googletrans import Translator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClNqN_6HL6sx"
      },
      "source": [
        "# Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0Lo5ym3Rdz_"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"aslg_pc12\")\n",
        "dataset = dataset[\"train\"].train_test_split(train_size=0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Back Translation"
      ],
      "metadata": {
        "id": "aiIIrUN0gGCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset[\"train\"].train_test_split(train_size=0.8)"
      ],
      "metadata": {
        "id": "TwrBho6Rhr-t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "source_lang = \"text\"\n",
        "target_lang = \"gloss\"\n",
        "\n",
        "\n",
        "def preprocess_function_backtrans(examples):\n",
        "    inputs = examples[source_lang]\n",
        "    targets = examples[target_lang]\n",
        "    model_inputs = tokenizer(inputs,  text_target=targets, max_length=128, truncation=True)\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_data = dataset.map(preprocess_function_backtrans, batched=True)\n"
      ],
      "metadata": {
        "id": "3htmt7n4gJSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XnKS2o1YgnE1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "e21bc5e1-ddc0-4868-ceb5-4e95b2bc9100"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10527' max='10527' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10527/10527 29:20, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.127600</td>\n",
              "      <td>0.742242</td>\n",
              "      <td>16.423700</td>\n",
              "      <td>18.208700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.892800</td>\n",
              "      <td>0.571618</td>\n",
              "      <td>21.720700</td>\n",
              "      <td>18.177200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.826900</td>\n",
              "      <td>0.530255</td>\n",
              "      <td>23.175600</td>\n",
              "      <td>18.175400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10527, training_loss=1.1417567611046413, metrics={'train_runtime': 1764.5821, 'train_samples_per_second': 95.434, 'train_steps_per_second': 5.966, 'total_flos': 1289437821468672.0, 'train_loss': 1.1417567611046413, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"transformer_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "dJd1FJwopmYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"fuyulinh04/transformer_model\")\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"fuyulinh04/transformer_model\")"
      ],
      "metadata": {
        "id": "KnNPD9plq1iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(\"translation\", model=model, tokenizer = tokenizer)\n",
        "\n",
        "prediction = []\n",
        "\n",
        "def testing_data():\n",
        "  for i in range(0, 8000): # 50% of the old dataset (about 17000 data points)\n",
        "    text = dataset[\"train\"][i][\"text\"]\n",
        "    prediction.append(translator(text))"
      ],
      "metadata": {
        "id": "YuEXTR1uq-wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_data()"
      ],
      "metadata": {
        "id": "g4JJB5iwrHGc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_dataset():\n",
        "  for i in range(8000): # Merge into dataset, if duplicate found => not adding\n",
        "    if(prediction[i] not in dataset[\"train\"][\"gloss\"]):\n",
        "      dataset[\"train\"][\"gloss\"].append(prediction[i])\n",
        "      dataset[\"train\"][\"text\"].append(dataset[\"train\"][i][\"text\"])\n",
        "\n",
        "merge_dataset() # Generate new augmented dataset"
      ],
      "metadata": {
        "id": "mmVlPue4_Ca-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da2zCMQjL6sy"
      },
      "source": [
        "# Training Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.push_to_hub(\"fuyulinh04/txt_to_gls_dts\")"
      ],
      "metadata": {
        "id": "V8lt9QFnKZnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNxlvU2BRgB-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"fuyulinh04/txt_to_gls_dts\")"
      ],
      "metadata": {
        "id": "Q7lNqz_vOsET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "umodtld8RjGx"
      },
      "outputs": [],
      "source": [
        "source_lang = \"gloss\"\n",
        "target_lang = \"text\"\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[source_lang]\n",
        "    targets = examples[target_lang]\n",
        "    model_inputs = tokenizer(inputs,  text_target=targets, max_length=128, truncation=True)\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V6NsgNuRk4E"
      },
      "outputs": [],
      "source": [
        "tokenized_data = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "tJFTJiYdRoGi"
      },
      "outputs": [],
      "source": [
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKetNX0RL6s0"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "MGFxX4WrL6s0"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHojBN0NL6s0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "outputId": "1a3ba210-7566-419c-e008-7efd23537fae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5915' max='10527' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 5915/10527 14:33 < 11:21, 6.77 it/s, Epoch 1.69/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.195600</td>\n",
              "      <td>0.816066</td>\n",
              "      <td>46.520500</td>\n",
              "      <td>15.691300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, gloss. If text, gloss are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 14034\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to transformer_model/checkpoint-4000\n",
            "Configuration saved in transformer_model/checkpoint-4000/config.json\n",
            "Model weights saved in transformer_model/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in transformer_model/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in transformer_model/checkpoint-4000/special_tokens_map.json\n",
            "Deleting older checkpoint [transformer_model/checkpoint-2500] due to args.save_total_limit\n",
            "Saving model checkpoint to transformer_model/checkpoint-4500\n",
            "Configuration saved in transformer_model/checkpoint-4500/config.json\n",
            "Model weights saved in transformer_model/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in transformer_model/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in transformer_model/checkpoint-4500/special_tokens_map.json\n",
            "Deleting older checkpoint [transformer_model/checkpoint-3000] due to args.save_total_limit\n",
            "Saving model checkpoint to transformer_model/checkpoint-5000\n",
            "Configuration saved in transformer_model/checkpoint-5000/config.json\n",
            "Model weights saved in transformer_model/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in transformer_model/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in transformer_model/checkpoint-5000/special_tokens_map.json\n",
            "Deleting older checkpoint [transformer_model/checkpoint-3500] due to args.save_total_limit\n",
            "Saving model checkpoint to transformer_model/checkpoint-5500\n",
            "Configuration saved in transformer_model/checkpoint-5500/config.json\n",
            "Model weights saved in transformer_model/checkpoint-5500/pytorch_model.bin\n",
            "tokenizer config file saved in transformer_model/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in transformer_model/checkpoint-5500/special_tokens_map.json\n",
            "Deleting older checkpoint [transformer_model/checkpoint-4000] due to args.save_total_limit\n"
          ]
        }
      ],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"transformer_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "eaZwQhv_k3UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BLEU score when applying Back Translation is:"
      ],
      "metadata": {
        "id": "g2brgh_3xs4w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Ga1KKML6s1"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJRTU1WFRraj"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"junowhite/transformer_model\")\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"junowhite/transformer_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3RwZNc-L6s1"
      },
      "outputs": [],
      "source": [
        "translators = Translator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcHA_CSQU64B"
      },
      "outputs": [],
      "source": [
        "translator = pipeline(\"translation\", model=model, tokenizer = tokenizer)\n",
        "\n",
        "prediction = []\n",
        "answer = []\n",
        "\n",
        "def testing_data():\n",
        "  for i in range(7):\n",
        "    text = dataset[\"train\"][i][\"gloss\"]\n",
        "    answer.append(dataset[\"train\"][i][\"text\"])\n",
        "    prediction.append(translators.translate(translator(text)[0][\"translation_text\"]).text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDjerUwsxSOB"
      },
      "outputs": [],
      "source": [
        "testing_data()\n",
        "print(prediction)\n",
        "print(answer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "bea42e1b0e07028483ba0ff26b9b4dc4fa162e9d0ccb6b0507d54b9d42d30653"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
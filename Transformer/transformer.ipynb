{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ! pip install huggingface_hub"
      ],
      "metadata": {
        "id": "fg3nrh3VnPug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRlcfbMMSCcT"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtkaMmL7RaX_"
      },
      "outputs": [],
      "source": [
        "! pip install transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0Lo5ym3Rdz_"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "\n",
        "dataset = load_dataset(\"aslg_pc12\")\n",
        "dataset = dataset[\"train\"].train_test_split(train_size=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNxlvU2BRgB-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "umodtld8RjGx"
      },
      "outputs": [],
      "source": [
        "source_lang = \"gloss\"\n",
        "target_lang = \"text\"\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[source_lang]\n",
        "    targets = examples[target_lang]\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=512, truncation=True)\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V6NsgNuRk4E"
      },
      "outputs": [],
      "source": [
        "tokenized_data = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiNd3MQbRmmt"
      },
      "outputs": [],
      "source": [
        "! pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJFTJiYdRoGi"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "70ZVLcdrRpmp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJRTU1WFRraj"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EyFef79PRs2R"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "_pKeByiHRuGX",
        "outputId": "57e60444-3694-4a17-ac8a-489e10f65893"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13158' max='13158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13158/13158 34:03, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.150900</td>\n",
              "      <td>0.743268</td>\n",
              "      <td>49.458700</td>\n",
              "      <td>15.711100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.940700</td>\n",
              "      <td>0.623432</td>\n",
              "      <td>54.908800</td>\n",
              "      <td>15.643700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.868700</td>\n",
              "      <td>0.594972</td>\n",
              "      <td>56.217300</td>\n",
              "      <td>15.625000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=13158, training_loss=1.1230986131725205, metrics={'train_runtime': 2046.9226, 'train_samples_per_second': 102.839, 'train_steps_per_second': 6.428, 'total_flos': 3208336615931904.0, 'train_loss': 1.1230986131725205, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"transformer_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install googletrans==3.1.0a0"
      ],
      "metadata": {
        "id": "1XIS8DiVwceF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator\n",
        "translators = Translator()\n"
      ],
      "metadata": {
        "id": "CFZXyUDqwe4o"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcHA_CSQU64B"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"transformer_model\")\n",
        "\n",
        "prediction = []\n",
        "answer = []\n",
        "\n",
        "def testing_data():\n",
        "  for i in range(20, 25, 1):\n",
        "    text = dataset[\"test\"][i][\"gloss\"]\n",
        "    answer.append(dataset[\"test\"][i][\"text\"])\n",
        "    prediction.append(translators.translate(translator(text)[0][\"translation_text\"]).text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testing_data()\n",
        "print(prediction)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wToyEea2yty6",
        "outputId": "dc220297-42ea-48ad-d3e8-1f376c1c33e3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Nicht ALIGNMENT WITH RESPECT to NATO is modern ALTERNATIVE for the mature state .', 'This is before the Huge TRAGEDY.', 'it is desired to intensify in the search and in nature.', 'i have many reviews relating to the UNHCR .', 'he does not express his concern on the committee .']\n",
            "['non alignment with respect to nato is the modern alternative for a mature state .\\n', 'this is therefore a huge tragedy .\\n', 'it is the first to suffer the increase in droughts and natural disasters .\\n', \"i have many reservations about the unhcr's overall credibility .\\n\", 'he did express his condolences on behalf of the commission .\\n']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}